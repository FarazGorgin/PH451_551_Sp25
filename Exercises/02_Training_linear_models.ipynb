{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FarazGorgin/PH451_551_Sp25/blob/main/Exercises/02_Training_linear_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWEunswqrzWp"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "File name convention: For group 42 and memebers Richard Stallman and <br>\n",
        "Linus Torvalds it would be <br>\n",
        "\"Exercise2_Goup42_Stallman_Torvalds.pdf\".<br>\n",
        "\n",
        "Submission via blackboard (UA).<br>\n",
        "\n",
        "Feel free to answer free text questions in text cells using markdown <br>\n",
        "and possibly $\\LaTeX{}$ if you want to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRSpWkx0rzWx"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrId8lV7rzWx"
      },
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots <br> figures inline and prepare a function to save the figures. We also <br> check that Python 3.5 or later is installed (although Python 2.x may <br>\n",
        "work, it is deprecated so we strongly recommend you use Python 3 <br> instead), as well as Scikit-Learn ≥0.20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_ZytlnYYrzWy"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3c5b04e9rzW0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HGEkqfDPrzW2",
        "outputId": "20d58986-3d40-4864-d758-32c1561677c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3XwzO8KBnX9"
      },
      "source": [
        "# Tasks 1-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PRAKeQ2rzW3"
      },
      "source": [
        "### Task 1\n",
        "Fit a linear [regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) `lin_reg` and print out its intercept and coefficient. <br>\n",
        "\n",
        "To start with, create a LinearRegression model object then use <br>\n",
        "`lin_reg.fit(X, y)` to fit the model to the data sets X and y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HEupw4u4rzW4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzACHvMk2hd6"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SZEguB-ArzW5",
        "outputId": "0d7b705d-f189-4c2e-9ae2-5ca3f926901f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept:  [4.21509616]\n",
            "Coefficient:  [[2.77011339]]\n"
          ]
        }
      ],
      "source": [
        "# lin_reg =\n",
        "lin_reg=LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "print (\"Intercept: \", lin_reg.intercept_)\n",
        "print(\"Coefficient: \", lin_reg.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCHZcxPr2tqY"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FptcwTMhrzW5"
      },
      "source": [
        "### Task 2\n",
        "With the model you created above, [predict](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) the new values `X_new`."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iAQeJRbOemUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aEYoEbYRrzW6"
      },
      "outputs": [],
      "source": [
        "X_new = np.array([[0], [2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjoh3xeA36Rc"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "22Frl1mtrzW7",
        "outputId": "6f5a4a8b-a4aa-403d-beb0-adb1841f9b17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.21509616]\n",
            " [9.75532293]]\n"
          ]
        }
      ],
      "source": [
        "y_predict = lin_reg.predict(X_new)\n",
        "print(y_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9UKK78g38bM"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5v1vgjirzW7"
      },
      "source": [
        "Let's plot the data (blue) and our predictions (red) using matplotlib.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OV7ZcG7PrzW8",
        "scrolled": true,
        "outputId": "eb2e53b8-e4f9-4646-97c5-c6fb6f6c1341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(X_new, y_predict, \"r-\", markersize = 20)\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSBzsnlrfRua"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq0SLqyyrzW9"
      },
      "source": [
        "### Task 3\n",
        "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` <br>\n",
        "function (the name stands for \"least squares\"). In essence, it attempts <br>\n",
        "to fit a line by minimize the sum of squares of residuals (true - pred) <br>\n",
        "for all points in a data set.\n",
        "\n",
        "Basically we fit to the form $y = 1 \\Theta_0 + \\Theta_1 x = x_b\\cdot \\Theta$ with $x_b = (1, x)^T$ and <br>\n",
        "$\\Theta = (\\Theta_0, \\Theta_1)^T$. So we need to add a 1 to each instance of `X`. See slide 13 of <br>\n",
        "Lecture 4.\n",
        "\n",
        "Look up [`np.linalg.lstsq()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html), call it directly on `X_b`, `y` and print out the $\\Theta$ you <br>\n",
        "found this way. Compare the output with the output from your <br> `LinearRegression` model above. Set `rcond=1e-6` in `np.linalg.lstsq`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pulqnD5VrzW9",
        "outputId": "06244a1c-b520-48a9-de1c-e40c1dc88171",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.74908024]\n",
            " [1.         1.90142861]\n",
            " [1.         1.46398788]\n",
            " [1.         1.19731697]\n",
            " [1.         0.31203728]\n",
            " [1.         0.31198904]\n",
            " [1.         0.11616722]\n",
            " [1.         1.73235229]\n",
            " [1.         1.20223002]\n",
            " [1.         1.41614516]\n",
            " [1.         0.04116899]\n",
            " [1.         1.9398197 ]\n",
            " [1.         1.66488528]\n",
            " [1.         0.42467822]\n",
            " [1.         0.36364993]\n",
            " [1.         0.36680902]\n",
            " [1.         0.60848449]\n",
            " [1.         1.04951286]\n",
            " [1.         0.86389004]\n",
            " [1.         0.58245828]\n",
            " [1.         1.22370579]\n",
            " [1.         0.27898772]\n",
            " [1.         0.5842893 ]\n",
            " [1.         0.73272369]\n",
            " [1.         0.91213997]\n",
            " [1.         1.57035192]\n",
            " [1.         0.39934756]\n",
            " [1.         1.02846888]\n",
            " [1.         1.18482914]\n",
            " [1.         0.09290083]\n",
            " [1.         1.2150897 ]\n",
            " [1.         0.34104825]\n",
            " [1.         0.13010319]\n",
            " [1.         1.89777107]\n",
            " [1.         1.93126407]\n",
            " [1.         1.6167947 ]\n",
            " [1.         0.60922754]\n",
            " [1.         0.19534423]\n",
            " [1.         1.36846605]\n",
            " [1.         0.88030499]\n",
            " [1.         0.24407647]\n",
            " [1.         0.99035382]\n",
            " [1.         0.06877704]\n",
            " [1.         1.8186408 ]\n",
            " [1.         0.51755996]\n",
            " [1.         1.32504457]\n",
            " [1.         0.62342215]\n",
            " [1.         1.04013604]\n",
            " [1.         1.09342056]\n",
            " [1.         0.36970891]\n",
            " [1.         1.93916926]\n",
            " [1.         1.55026565]\n",
            " [1.         1.87899788]\n",
            " [1.         1.7896547 ]\n",
            " [1.         1.19579996]\n",
            " [1.         1.84374847]\n",
            " [1.         0.176985  ]\n",
            " [1.         0.39196572]\n",
            " [1.         0.09045458]\n",
            " [1.         0.65066066]\n",
            " [1.         0.77735458]\n",
            " [1.         0.54269806]\n",
            " [1.         1.65747502]\n",
            " [1.         0.71350665]\n",
            " [1.         0.56186902]\n",
            " [1.         1.08539217]\n",
            " [1.         0.28184845]\n",
            " [1.         1.60439396]\n",
            " [1.         0.14910129]\n",
            " [1.         1.97377387]\n",
            " [1.         1.54448954]\n",
            " [1.         0.39743136]\n",
            " [1.         0.01104423]\n",
            " [1.         1.63092286]\n",
            " [1.         1.41371469]\n",
            " [1.         1.45801434]\n",
            " [1.         1.54254069]\n",
            " [1.         0.1480893 ]\n",
            " [1.         0.71693146]\n",
            " [1.         0.23173812]\n",
            " [1.         1.72620685]\n",
            " [1.         1.24659625]\n",
            " [1.         0.66179605]\n",
            " [1.         0.1271167 ]\n",
            " [1.         0.62196464]\n",
            " [1.         0.65036664]\n",
            " [1.         1.45921236]\n",
            " [1.         1.27511494]\n",
            " [1.         1.77442549]\n",
            " [1.         0.94442985]\n",
            " [1.         0.23918849]\n",
            " [1.         1.42648957]\n",
            " [1.         1.5215701 ]\n",
            " [1.         1.1225544 ]\n",
            " [1.         1.54193436]\n",
            " [1.         0.98759119]\n",
            " [1.         1.04546566]\n",
            " [1.         0.85508204]\n",
            " [1.         0.05083825]\n",
            " [1.         0.21578285]]\n"
          ]
        }
      ],
      "source": [
        "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
        "print(X_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5IeVa0P6k0E"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5Jphg9m57kvE",
        "outputId": "cc15350a-f1cc-4d42-8307-3e055d5d1644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.21509616]\n",
            " [2.77011339]]\n"
          ]
        }
      ],
      "source": [
        "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
        "print(theta_best_svd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o9Bq2js73A4"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RG9puAEZ6anz",
        "outputId": "0fd8e3d6-4736-46c0-d400-eac0fbac86c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([4.21509616]), array([[2.77011339]]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# these are the values from the linear regression model\n",
        "# compare them to what you got above\n",
        "lin_reg.intercept_, lin_reg.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQfG16MYrzW-"
      },
      "source": [
        "# Linear regression using batch gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CLJrRFErzW-"
      },
      "source": [
        "Just click through this section.\n",
        "\n",
        "The first part shows how to manually do gradient descent. The formula <br>\n",
        "for the gradients has been calculated by hand and we just plug in `X`<br>\n",
        "and `theta`.\n",
        "\n",
        "The second part (function `plot_gradient_descent`) does the gradient <br>\n",
        "descent and plots the first 10 steps.\n",
        "\n",
        "[Here](https://github.com/Jaewan-Yun/optimizer-visualization) is a nice visualization of different gradient descent methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WUZNzC6grzW_",
        "outputId": "772f29b5-298b-42c1-9185-096bb55abab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.01300189]\n",
            " [1.45353408]]\n"
          ]
        }
      ],
      "source": [
        "eta = 0.1  # learning rate\n",
        "n_iterations = 1000 # number of iterations\n",
        "m = 100 # the number of items in X_b\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "print(theta) # printing what our initial theta values look like\n",
        "\n",
        "# The following loop is the entire gradient descent algorithm for this\n",
        "# linear regression example. As a bonus question below you can attempt to\n",
        "# describe what is actually happening here.\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients\n",
        "\n",
        "# Answer to the Bonus Question:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXp-XEM-JLSN"
      },
      "source": [
        "Notice the change in our value for theta after running this algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Bw5Wt52LrzW_",
        "outputId": "c40cad70-4d33-4947-97f7-04c02f23af05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.21509616],\n",
              "       [2.77011339]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O1NbQtXJRJq"
      },
      "source": [
        "Don't worry too much about what all of the code below is doing. In <br>\n",
        "short, it's **running the gradient descent algorithm and updating a plot** <br>\n",
        "with increasingly better-fitted regression lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyHa0cgrrzXA"
      },
      "outputs": [],
      "source": [
        "theta_path_bgd = []\n",
        "\n",
        "def plot_gradient_descent(theta, eta, theta_path=None):\n",
        "    m = len(X_b)\n",
        "    plt.plot(X, y, \"b.\")\n",
        "    n_iterations = 1000\n",
        "    X_new = np.array([[0], [2]])\n",
        "    X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
        "    for iteration in range(n_iterations):\n",
        "        if iteration < 10:\n",
        "            y_predict = X_new_b.dot(theta)\n",
        "            style = \"b-\" if iteration > 0 else \"r--\"\n",
        "            plt.plot(X_new, y_predict, style)\n",
        "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)   # this line\n",
        "        theta = theta - eta * gradients   # and this one\n",
        "        if theta_path is not None:\n",
        "            theta_path.append(theta)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 2, -10, 15])\n",
        "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS1iC02wKqYy"
      },
      "source": [
        "Now that we've made a function to do gradient descent and plot our <br>\n",
        "regressions, we can **test out a few different learning rate** <br>\n",
        "**hyper-parameters**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9UP42oZrzXA"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42) # this keeps your random outputs consistent each time\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "theta_path_bgd = []\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
        "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXB606II9NZK"
      },
      "source": [
        "For a learning rate $\\eta$ that is too small we approach the minimum too slowly and for a too large learning rate we jump around the minimum like the purple curve in [this animation](https://github.com/Jaewan-Yun/optimizer-visualization/blob/master/figures/movie9.gif) from [this repo](https://github.com/Jaewan-Yun/optimizer-visualization) on GitHub.\n",
        "Note that this animation does not correspond to our data.\n",
        "![animation](https://github.com/Jaewan-Yun/optimizer-visualization/blob/master/figures/movie9.gif?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfqxCe9WRtKv"
      },
      "source": [
        "In the above plot the axis of the plane could represent two different model <br>\n",
        "**parameters** and the vertical axis represents your **\"Loss\"** or measure of error. <br>\n",
        "As you move the points around across the plane you might **increase** or **decrease** <br>\n",
        "the loss. So when you are at the bottom of one of the valleys you've found a <br>\n",
        "**combination of parameters** (a point along the plane) that result in a low <br>\n",
        "loss. Gradient Descent and other optimization algorithms help you to take\n",
        "<br>\n",
        "steps in a direction that gets this point to the **lowest valley possible**. <br>\n",
        "The point along the plane that corresponds to being in the **deepest valley** <br>\n",
        "possible represents the **best** combination of model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pPLGUcLDB_w"
      },
      "source": [
        "### Task 3.5 (Bonus)\n",
        "Explain what happens in the two lines of code\n",
        "```python\n",
        "gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)   # this line\n",
        "theta = theta - eta * gradients   # and this one\n",
        "```\n",
        "in the code above. Also explain where the first line comes from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLQeFXCqDkhK"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4SGu63sDuHO"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 3.5 (bonus) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a-Q3Y0VDr1P"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA1ZmZKvrzXA"
      },
      "source": [
        "# Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amR_zg9OrzXB"
      },
      "source": [
        "This section shows how one can implement Stochastic Gradient Descent by <br>\n",
        "hand. You might notice that **this looks a lot like the gradient descent** <br>\n",
        "**method** from above. The difference here, is that **we're adding stochasticity** <br>\n",
        "**or randomness**. We do this by **performing gradient descent on a random** <br>\n",
        "**subset** of all data points for each step **rather than on the entire data** <br>\n",
        "**set** every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-8RC0w2rzXB"
      },
      "outputs": [],
      "source": [
        "theta_path_sgd = [] # a list where we can store parameters of our fit\n",
        "m = len(X_b) # the number of items in our data set\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-exggOfRQoA"
      },
      "source": [
        "Notice the lines below that are marked **#here**. Those lines are where **we** <br>\n",
        "**introduce stochasticity by picking out a random single data point**. <br>\n",
        "**Don't worry too much about whether you understand what's happening in** <br>\n",
        "**this code**. In practice, we'll usually use other tools to implement this <br>\n",
        "automatically rather than code it by hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpdXF8RGrzXB"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50 # number of times to fully iterate over our data set\n",
        "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
        "\n",
        "# Sometimes we want to be able to change our learning rate over time.\n",
        "# If we start with a learning rate of 1. Our next value would be 5/(1+50).\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "X_new = np.array([[0], [2]])\n",
        "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(m):\n",
        "        if epoch == 0 and i < 20:\n",
        "            y_predict = X_new_b.dot(theta)\n",
        "            style = \"b-\" if i > 0 else \"r--\"\n",
        "            plt.plot(X_new, y_predict, style)\n",
        "        random_index = np.random.randint(m)   #here\n",
        "        xi = X_b[random_index:random_index+1] #here\n",
        "        yi = y[random_index:random_index+1]   #here\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(epoch * m + i) # reduce our learning rate\n",
        "        theta = theta - eta * gradients\n",
        "        theta_path_sgd.append(theta)\n",
        "\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB09p83qrzXB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW3PHeIyrzXC"
      },
      "source": [
        "### Task 4\n",
        "- Build an [SGD Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) and assign it to `sgd_reg`.\n",
        "- Fit the SGD Regressor `sgd_reg` to `X` and `y` and print out its <br> intercept `sgd_reg.intercept_`and coefficients `sgd_reg.coef_`.\n",
        "- Compare the values you find to the ones from the stochastic gradient <br>\n",
        "descent above.\n",
        "\n",
        "Use `max_iter=50, tol=None, penalty=None, eta0=0.1, random_state=42` as parameters for the SGD Regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-ZAT9jlrzXC"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwvYd3w4FUkC"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7uzxyparzXC"
      },
      "outputs": [],
      "source": [
        "# sgd_reg ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgEgY7NUFfwu"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGYI3I2IrzXC"
      },
      "source": [
        "# Mini-batch gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1C9EIZxrzXD"
      },
      "source": [
        "The following code shows how one could implement mini-batch gradient <br>\n",
        "descent by hand. **Remember, the stochastic part of SGD means we picked** <br>\n",
        "**out a random subset of our data**. Above, that was only a single point. <br>\n",
        "How do you think mini-batch gradient descent differs from stochastic <br>\n",
        "gradient descent? As a hint, look for `#pay close attention to this line`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDyGZv2TrzXD"
      },
      "outputs": [],
      "source": [
        "theta_path_mgd = []\n",
        "\n",
        "n_iterations = 50\n",
        "minibatch_size = 20\n",
        "\n",
        "np.random.seed(42)\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "t0, t1 = 200, 1000\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "t = 0\n",
        "for epoch in range(n_iterations):\n",
        "    shuffled_indices = np.random.permutation(m)\n",
        "    X_b_shuffled = X_b[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    for i in range(0, m, minibatch_size):\n",
        "        t += 1\n",
        "        xi = X_b_shuffled[i:i+minibatch_size] #pay close attention to this line\n",
        "        yi = y_shuffled[i:i+minibatch_size]   #pay close attention to this line\n",
        "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(t)\n",
        "        theta = theta - eta * gradients\n",
        "        theta_path_mgd.append(theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN-WG-virzXD"
      },
      "outputs": [],
      "source": [
        "theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY6CkmGbrzXD"
      },
      "outputs": [],
      "source": [
        "theta_path_bgd = np.array(theta_path_bgd)\n",
        "theta_path_sgd = np.array(theta_path_sgd)\n",
        "theta_path_mgd = np.array(theta_path_mgd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zOmS8USUMyU"
      },
      "source": [
        "Let's see how these three different approaches to gradient descent <br>\n",
        "compare with one another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAlRIsEdrzXE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1, label=\"Stochastic\")\n",
        "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
        "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\n",
        "plt.legend(loc=\"upper left\", fontsize=16)\n",
        "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
        "plt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\n",
        "plt.axis([2.5, 4.5, 2.3, 3.9])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvPe8JMrzXE"
      },
      "source": [
        "### Task 5\n",
        "Explain which Linear Regression training algorithm you can use if you <br>\n",
        "have a training set with millions of features? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HJaiUFt7Usg"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWjF5XmDEtQW"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 5 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjB4sX9KFlE5"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcV_jYLLrzXF"
      },
      "source": [
        "# Polynomial regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn3PrphGrzXE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD47wIkqrzXF"
      },
      "source": [
        "With a little trick Linear Regression turns out to be rather powerful <br> also for data that is not linear! <br>\n",
        "The trick is to **add \"new features\" to `X`**. In our case we need a second <br>\n",
        "feature that is just $x^2$. **Then we fit again**, but because **our feature**<br>\n",
        "**vector is now $(1, x, x^2)$**, the **dot product with** $(\\Theta_0, \\Theta_1, \\Theta_2)$ **gives** <br>\n",
        "**the form for the parabola** (check this if you don't see it!).\n",
        "\n",
        "Here we show an example of how to fit a parabola with Linear Regression.\n",
        "\n",
        "**You do not need to understand the code** in detail but we've added <br>\n",
        "comments if you want to understand it better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1uyl-1KrzXF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
        "\n",
        "# From the documentation for PolynomialFeatures:\n",
        "# Generate a new feature matrix consisting of all polynomial combinations of\n",
        "# the features with degree less than or equal to the specified degree. For\n",
        "# example, if an input sample is two dimensional and of the form [a, b], the\n",
        "# degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "# We can then fit to and transform our 100 X values to look like a polynomial.\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "# LinearRegression fits the polynomials with least squares fitting.\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "\n",
        "# We can also transform other sets of X values without needing to fit first\n",
        "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "\n",
        "# With some new polynomial X values that the model hasn't seen, we can get\n",
        "# some predicted values.\n",
        "y_new = lin_reg.predict(X_new_poly)\n",
        "\n",
        "# The following for loop is going to repeat for some polynomials of different\n",
        "# degrees and create a regression  plot with corresponding design elements.\n",
        "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
        "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    std_scaler = StandardScaler()\n",
        "    lin_reg = LinearRegression()\n",
        "    # Pipeline is a tool that lets you apply multiple sklearn layers to data\n",
        "    # in a sequential way. Here, we're transforming our data to be polynomial,\n",
        "    # then scaling the data using standard normalization, and finally running\n",
        "    # a linear regression on the data.\n",
        "    polynomial_regression = Pipeline([\n",
        "            (\"poly_features\", polybig_features),\n",
        "            (\"std_scaler\", std_scaler),\n",
        "            (\"lin_reg\", lin_reg),\n",
        "        ])\n",
        "    # with our Pipeline constructed, we can fit our model to some data.\n",
        "    polynomial_regression.fit(X, y)\n",
        "    y_newbig = polynomial_regression.predict(X_new)\n",
        "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
        "\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.title(\"Fitting different polynomials\")\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RX8MFNRH8l5"
      },
      "source": [
        "### Task 6\n",
        "- Which curve (red, blue, green) fits the data (blue dots) the best?\n",
        "- Explain what happens to the green curve.\n",
        "<br>Hint: (\"g-\", 1, 300) = style, width, degree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icSe9mOO7W74"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EMq8MKCE7_2"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 6 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e21Gcly27RC5"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KOQXfIBrzXF"
      },
      "source": [
        "# Training and Validation Error\n",
        "\n",
        "**Learning curves are a visualization of your error over time**. Here we're <br>\n",
        "looking at plots of mean-squared error (true-pred)^2 for both our <br>\n",
        "training data and our validation data over the course of training. By <br>\n",
        "**checking BOTH training and validation curves**, we can verify that our <br>\n",
        "model is **learning AND generalizing**.\n",
        "\n",
        "Not all machine learning toolkits keep track of your loss history <br>\n",
        "automatically. Here's an example doing it by hand with lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfE3AUIlrzXF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    # train_test_split splits data into train and test sets automatically\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "    train_errors, val_errors = [], []\n",
        "    for m in range(1, len(X_train)):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_val_predict = model.predict(X_val)\n",
        "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
        "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
        "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
        "    plt.legend(loc=\"upper right\", fontsize=14)\n",
        "    plt.xlabel(\"Training set size\", fontsize=14)\n",
        "    plt.ylabel(\"RMSE\", fontsize=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quH_toCnrzXG"
      },
      "source": [
        "Let's compare different degrees of polynomials for fitting. We're going <br>\n",
        "to **generate some learning curves for increasingly complicated** <br>\n",
        "**polynomial forms. Degree = 1, 2, 30**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngKw-o-ArzXG"
      },
      "outputs": [],
      "source": [
        "# Linear Regression (degree 1)\n",
        "lin_reg = LinearRegression()\n",
        "plot_learning_curves(lin_reg, X, y)\n",
        "plt.axis([0, 80, 0, 3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARFBQLB1rzXG"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Degree 2\n",
        "polynomial_regression = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=2, include_bias=True)),\n",
        "        (\"lin_reg\", LinearRegression()),\n",
        "    ])\n",
        "\n",
        "plot_learning_curves(polynomial_regression, X, y)\n",
        "plt.axis([0, 80, 0, 2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC9gEVpArzXG"
      },
      "outputs": [],
      "source": [
        "# Degree 20\n",
        "polynomial_regression = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=30, include_bias=True)),\n",
        "        (\"lin_reg\", LinearRegression()),\n",
        "    ])\n",
        "\n",
        "plot_learning_curves(polynomial_regression, X, y)\n",
        "plt.axis([0, 80, 0, 3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srSUIubHrzXH"
      },
      "source": [
        "### Task 7\n",
        "Look at the three plots above that show the training and validation <br>\n",
        "errors for linear regression, quadratic regression and order 20. <br> Compare them to the plot above called \"Fitting different polynomials\". <br>\n",
        "\n",
        "Describe which one is likely overfitting and which one is likely <br> underfitting and why.\n",
        "\n",
        "Hint: Also take the values of the errors into consideration when <br> comparing the different polynomials and not just the shape of the train <br>\n",
        "and validation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GjY75AyAWL_"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKPlcI1lI4D5"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 7 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpOfr3BKAcTy"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcHILVuOrzXH"
      },
      "source": [
        "# Regularized models\n",
        "\n",
        "**Regularization** is one approach to help prevent overfitting. The general <br>\n",
        "idea is to **apply a penalty to model coefficients which discourages** <br>\n",
        "**overly-complex models**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c-JnVESrzXH"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "m = 20\n",
        "X = 3 * np.random.rand(m, 1)\n",
        "y = 1 + X*X + 1.5 * np.random.randn(m, 1) / 2\n",
        "X_new = np.linspace(0, 3, 1000).reshape(1000, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdD6Mv1rzXM"
      },
      "source": [
        "### Task 8\n",
        "**Ridge regression is one such form of regularized regression**.\n",
        "\n",
        "Create a [ridge linear model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with `alpha=1` and Stochastic Average Gradient <br>\n",
        "descent (`solver=\"sag\"`) solver to `X` and `y`. <br>\n",
        "Fit (`.fit()`) your model to `X` and `y`.<br>\n",
        "Check which value it predicts (`.predict()`) for `X=[[1.5]]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BToG8SUwrzXM"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VALJg_UBHQr"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M1mEPGfrzXM"
      },
      "outputs": [],
      "source": [
        "# ridge_reg_sag ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRMGJ7M9BKKb"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J_YgMtxrzXN"
      },
      "source": [
        "Now we fit a linear model (left plot) and a degree 15 polynomial model <br>\n",
        "(right plot) with ridge regression for different values of alpha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXJOazhMrzXN"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Most of the following function should loook familiar from other fit and plot\n",
        "# loops in this hands-on\n",
        "def plot_model(model_class, polynomial, alphas, **model_kargs):\n",
        "    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n",
        "        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
        "        if polynomial:\n",
        "            model = Pipeline([\n",
        "                    (\"poly_features\", PolynomialFeatures(degree=15, include_bias=False)),\n",
        "                    (\"std_scaler\", StandardScaler()),\n",
        "                    (\"regul_reg\", model),\n",
        "                ])\n",
        "        model.fit(X, y)\n",
        "        y_new_regul = model.predict(X_new)\n",
        "        lw = 2 if alpha > 0 else 1\n",
        "        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n",
        "    plt.plot(X, y, \"b.\", linewidth=3)\n",
        "    plt.legend(loc=\"upper left\", fontsize=15)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 3, 0, 10])\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(122)\n",
        "plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCDx4VkCJeLz"
      },
      "source": [
        "### Task 9\n",
        "- Describe the effect of the regularization parameter alpha in the plot <br> on the right hand side.\n",
        "- Which curve (blue, green, red) would you expect to have the smallest <br>\n",
        "training loss?\n",
        "- Which curve would you expect to generalize the best to new data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK4kWJQqLHck"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS1HfH6aLIyA"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 9 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r9Ne4pkLIgi"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_xo-LzfuWIm"
      },
      "source": [
        "We can also add regularization penalties as arguments to other sklearn <br>\n",
        "model classes. **Below we'll show a couple of other ways to add** <br>\n",
        "**regularization to your model training**.\n",
        "\n",
        "L2 = Ridge regularization = square value regularization<br>\n",
        "L1 = Lasso regularization = absolute value regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYjBx6ourzXN"
      },
      "outputs": [],
      "source": [
        "sgd_reg = SGDRegressor(penalty=\"l2\", max_iter=1000, tol=1e-3, random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "sgd_reg.predict([[1.5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94IZ_L8urzXN"
      },
      "source": [
        "The same, but with Lasso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQSD89h_rzXN"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(122)\n",
        "plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), random_state=42)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujSNU_ferzXO"
      },
      "source": [
        "### Task 10\n",
        "create a Lasso linear model with `alpha=1` and assign it to <br> `lasso_reg`. Fit the model to X and y using `.fit()`. Check which value <br>\n",
        "it predicts for `X=[[1.5]]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APlgquD2rzXO"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_N97bVrEP1Q"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcInIrjgrzXO"
      },
      "outputs": [],
      "source": [
        "# lasso_reg ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxjZzo2YEUEx"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaXbwrujrzXP"
      },
      "source": [
        "## Early stopping example\n",
        "\n",
        "**Sometimes our final model at the end of training is not as good as the** <br>\n",
        "**best model from the whole training procedure**. This could be true if, <br>\n",
        "for example, your model overfits to the training data. **Below we'll show** <br>\n",
        "**how you can save the best model from all of training with sklearn**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSNv0btDrzXP"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hYP8Ti5rzXP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "poly_scaler = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
        "        (\"std_scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=None, warm_start=True,\n",
        "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    if val_error < minimum_val_error: # Check if this is our best model\n",
        "        minimum_val_error = val_error # Overwrite the minimum error\n",
        "        best_epoch = epoch # Record the best epoch\n",
        "\n",
        "        # Notice how we save a deep copy of the model instead of just writing\n",
        "        # best_model = sgd_reg. This is because assigning a variable to a\n",
        "        # different variable can sometimes cause python to treat them as one\n",
        "        # object; meaning changes made to one would affect the other.\n",
        "        best_model = deepcopy(sgd_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lECmhOKwyyj1"
      },
      "source": [
        "We can also do something similar when searching for good hyper- <br> parameters (e.g. number of epochs) by **saving our loss history instead** <br>\n",
        "**of saving the best model**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEiOzmTjrzXP"
      },
      "outputs": [],
      "source": [
        "sgd_reg = SGDRegressor(max_iter=1, tol=None, warm_start=True,\n",
        "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "n_epochs = 500\n",
        "train_errors, val_errors = [], []\n",
        "for epoch in range(n_epochs):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
        "    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
        "    val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "best_epoch = np.argmin(val_errors)\n",
        "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
        "\n",
        "plt.annotate('Best model',\n",
        "             xy=(best_epoch, best_val_rmse),\n",
        "             xytext=(best_epoch, best_val_rmse + 1),\n",
        "             ha=\"center\",\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "             fontsize=16,\n",
        "            )\n",
        "\n",
        "best_val_rmse -= 0.03  # just to make the graph look better\n",
        "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
        "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\n",
        "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\n",
        "plt.legend(loc=\"upper right\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.ylabel(\"RMSE\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLty71JWrzXQ"
      },
      "source": [
        "### Task 11\n",
        "Suppose you use Batch Gradient Descent and you plot the validation <br> error at every epoch. If you notice that the validation error <br>\n",
        "consistently goes up, what is likely going on? <br>\n",
        "What would be options for fixing this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBxdA9gcEjYo"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiQ8itC_LsLC"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 11 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXsEtJj3Egno"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okk_0qAcrzXQ"
      },
      "source": [
        "### Task 12\n",
        "We want to find at which exact epoch our model is the best. <br>\n",
        "For that we go through 1000 \"epochs\".\n",
        "\n",
        "We have already prepared part of the code for this task. You only have <br>\n",
        "to fill in the last part which should do the following:\n",
        "\n",
        "- if the validation error (of the current epoch) is smaller than the <br> smallest validation error until now (`minimum_val_error`), then:\n",
        "    - set the `minimum_val_error` to be the current validation error\n",
        "    - set the `best_epoch` to be the current epoch\n",
        "    - set the `best_model` to be the current model (`clone(sgd_reg)`)\n",
        "- else: next epoch (you can also just leave out `else`)\n",
        "\n",
        "**You can check the early stop example above if you're unsure how to do <br>\n",
        "this.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGsvUE8JrzXQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import clone\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=None, warm_start=True, penalty=None,\n",
        "                       learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    # ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below\n",
        "\n",
        "    # ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this\n",
        "\n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drZACu6vMYVz"
      },
      "source": [
        "Task 12.5 bonus question: Why do we need `clone(sgd_reg)` here and not <br>\n",
        "just `sgd_reg`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHpMfph7M0-c"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImSMkULxMgP9"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 12.5 (bonus) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO-6REp_M3ul"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqqN4XQ5rzXQ"
      },
      "source": [
        "# Logistic regression\n",
        "\n",
        "Below we'll see an example of a model that's sometimes referred to as a <br>\n",
        "**logistic regression classifier**. The model essentially tries to map <br>\n",
        "multiple variables to the log-odds of a particular class being selected <br>\n",
        "in such a way that the log-odds is a linear combination of the <br>\n",
        "variables.\n",
        "\n",
        "Note: logistic regression models include:\n",
        "- maximum entropy models (the multi-class case)\n",
        "- binomial logistic regression models (the binary case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgmxSMiprzXR"
      },
      "outputs": [],
      "source": [
        "t = np.linspace(-10, 10, 100)\n",
        "sig = 1 / (1 + np.exp(-t))\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.plot([-10, 10], [0, 0], \"k-\")\n",
        "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
        "plt.plot([-10, 10], [1, 1], \"k:\")\n",
        "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
        "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.legend(loc=\"upper left\", fontsize=20)\n",
        "plt.axis([-10, 10, -0.1, 1.1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q175BB8YM_4e"
      },
      "source": [
        "From here on we will work with the [Iris Flower Data Set](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
        "\n",
        "First, let's import that dataset from sklearn's collection of datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCGkSSy2rzXR"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NYUu3OerzXR"
      },
      "outputs": [],
      "source": [
        "print(iris.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AGHNbX4OHWk"
      },
      "source": [
        "Let's only use one feature: The petal width\n",
        "\n",
        "`iris[\"data\"][:, 3:]` is getting the values for petal width.\n",
        "\n",
        "We need to convert our labels to integers (they're strings right now)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJnA7NulrzXR"
      },
      "outputs": [],
      "source": [
        "X = iris[\"data\"][:, 3:]  # petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.int32)  # 1 if Iris virginica, else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo6fNNuO3yyV"
      },
      "source": [
        "Let's create a logistic regression model and fit it to our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUEwUjZerzXR"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "log_reg.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decision_boundary)"
      ],
      "metadata": {
        "id": "owUg2dSYvocC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbwNtyS6rzXS"
      },
      "outputs": [],
      "source": [
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0][0]\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(X[y==0], y[y==0], \"bs\")\n",
        "plt.plot(X[y==1], y[y==1], \"g^\")\n",
        "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
        "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
        "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")\n",
        "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
        "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
        "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
        "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
        "plt.ylabel(\"Probability\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 3, -0.02, 1.02])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNS8yXYRrzXS"
      },
      "outputs": [],
      "source": [
        "decision_boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHnfuLBlOWj_"
      },
      "source": [
        "The decision boundary is at Petal width = 1.66cm. <br>\n",
        "Let's see what we get if we predict the class of an instance slightly <br>\n",
        "to the left (1.5) and one slightly to the right (1.7)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy-BKAq2NZAK"
      },
      "outputs": [],
      "source": [
        "# if you are motivated: Try to understand this output\n",
        "log_reg.predict_proba([[1.7], [1.5]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5I1RefjrzXS"
      },
      "outputs": [],
      "source": [
        "log_reg.predict([[1.7], [1.5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MobMBsLOvGb"
      },
      "source": [
        "### Task 13\n",
        "What is the class prediction (iris virginica or not iris virginica?) <br>\n",
        "for petal width = 1.7 and what is the prediction for petal width = 1.5?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ola1BG00PINb"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxOueRiLPI6p"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 13 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abO95ce6PKrO"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkKGQwlMPRM5"
      },
      "source": [
        "Now let's use two features for predicting the class: <br>\n",
        "The petal length and the petal width. <br>\n",
        "We also use all classes this time and not just two like above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tp3V7ukBnYP"
      },
      "source": [
        "## Extension to 2 dimensions\n",
        "There are no tasks for this part. You can try to understand what <br>\n",
        "happens or skip if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo0ZrnDKrzXS"
      },
      "outputs": [],
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
        "softmax_reg.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "augxRMiGrzXS"
      },
      "outputs": [],
      "source": [
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
        "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "\n",
        "y_proba = softmax_reg.predict_proba(X_new)\n",
        "y_predict = softmax_reg.predict(X_new)\n",
        "\n",
        "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 7, 0, 3.5])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEL3UyOgrzXT"
      },
      "outputs": [],
      "source": [
        "softmax_reg.predict([[5, 2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYq9mMx1rzXT"
      },
      "outputs": [],
      "source": [
        "softmax_reg.predict_proba([[5, 2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn-Qmqd9BnYQ"
      },
      "source": [
        "## General Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBGj1xiqrzXT"
      },
      "source": [
        "### Task 14\n",
        "Can Gradient Descent get stuck in a local minimum when training a <br> Logistic Regression model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnmNY_mlQJ_y"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpat0HnMrzXT"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 14 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW6VxC40QMZo"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7liesyzIrzXT"
      },
      "source": [
        "### Task 15\n",
        "Which Gradient Descent algorithm (among those we discussed) will reach <br>\n",
        "the vicinity of the optimal solution the fastest? Which will actually <br>\n",
        "converge? How can you make the others converge as well?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGyb_lSkQPZa"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRt4Sv5JrzXT"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 15 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC9eg3tOQOD2"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}